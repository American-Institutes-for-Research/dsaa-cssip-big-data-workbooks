{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - Initialize packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"../modules/orcid-python\")\n",
    "sys.path.append(\"../modules/pyalm\")\n",
    "import time\n",
    "import orcid\n",
    "import pyalm.pyalm as pyalm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues in practice - gathering and integrating data from multiple sources\n",
    "\n",
    "It is common to work across multiple data sources to gather information. A very common pattern is to to search in one location to create a list of identifiers and then use those identifiers to query another API. In the ORCID example above we created a list of DOIs from a single ORCID profile. We could use those DOIs to obtain further information from the Crossref API and other sources. This models a common path for analysis of research outputs: identifying a corpus and then seeking information on its performance.\n",
    "\n",
    "In this example we will build on the ORCID and Crossref examples to collect a set of work identifiers from an ORCID profile and use a range of APIs to identify additional metadata as well as information on the performance of those articles. In addition to the ORCID API we will use the PLOS Lagotto API. Lagotto is the software that was built to support the Article Level Metrics program at PLOS, the Open Access publisher, and its API provides information on various metrics of PLOS articles. A range of other publishers and service providers, including Crossref also provide an instance of this API meaning the same tools can be used to collect information on articles from a range of sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Lagotto API\n",
    "\n",
    "The module `pyalm` is a wrapper for the Lagotto API which is served from a range of hosts. In this we will work with instances run by PLOS and by Crossref (the `det` instance). We first need to provide the details of the URLs for these instances to our wrapper. Then we can obtain some information for a single DOI to see what the data returned looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyalm.config.APIS = { 'plos' : {'url': 'http://alm.plos.org/api/v5/articles'},\n",
    "                      'det'  : {'url' : 'http://det.labs.crossref.org/api/v5/articles'}\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "det_alm_test = pyalm.get_alm('10.1371/journal.pbio.1001677', info='detail', instance='det')\n",
    "det_alm_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library returns a python dictionary containing two elements. The `articles` key contains the actual data and the `meta` key includes general information on the results of the interaction with the API. In this case it has returned one page of results containing one object (because we only asked about one DOI). If we want to collect a lot of data this information helps in the process of paging through results. It is common for APIs to impose some limit on the number of results returned so as to ensure performance. By default the Lagotto API has a limit of 50 results.\n",
    "\n",
    "The `articles` key holds a list of `ArticleALM` objects as its value. Each ArticleALM object has a set of internal attributes that contain information on each of the metrics that PLOS collects. These are derived from various data providers and are called 'sources'. Each can be accessed by name from a dictionary called 'sources'. The iterkeys() function provides an interator that lets us loop over the set of keys in a dictionary. Within the source object there is a range of information that we will dig into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article = det_alm_test.get('articles')[0]\n",
    "article.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for source in article.sources.iterkeys():\n",
    "    print source, article.sources[source].metrics.total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DET service only has a record of citations to this article from Wikipedia. As we will see below the PLOS service returns more results. This is because some of the sources are not yet being queried by DET.  \n",
    "\n",
    "Because this is a PLOS paper we can also query the PLOS Lagotto instance for the same article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plos_alm_test = pyalm.get_alm('10.1371/journal.pbio.1001677', info='detail', instance='plos')\n",
    "article_plos = plos_alm_test.get('articles')[0]\n",
    "article_plos.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for source in article_plos.sources.iterkeys():\n",
    "    print source, article_plos.sources[source].metrics.total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PLOS instance is providing a greater range of information but also seems to be giving larger numbers than the DET instance in many cases as well. For those sources that are provided by both API instances we can compare the results returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for source in article.sources.iterkeys():\n",
    "    print source, article.sources[source].metrics.total, article_plos.sources[source].metrics.total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PLOS Lagotto instance is both collecting more information and has a wider range of information sources. Comparing the results from the PLOS and DET instances illustrates the issues of coverage and completeness discussed previously. The data may be sparse for a variety of reasons and it is important to have a clear idea of the strengths and weaknesses of a particular data source or aggregator. In this case the DET instance is returning information for some sources which it is does not have data for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can dig deeper into the events themselves that the metrics.total count aggregates. The API wrapper collects these into an event object within the source object. These contain the JSON returned from the API in most cases. For instance the Crossref source is a list of JSON objects containing information on an article that cites our article of interest. The first citation event in the list is a citation from the journal JASIST by Du et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_plos.sources['crossref'].events[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another source in the PLOS data is Twitter. In the case of the twitter events (individual tweets) this provides the text of the tweet, user ids, user names, url of the tweet and the date. We can see from the length of the events list that there are at least 130 tweets that link to this article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(article_plos.sources['twitter'].events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, noting the issues of coverage, scope and completeness it is important to consider the limitations of this data. This is a lower bound as it represents search results returned by search the Twitter API for the DOI or URL of the article. Other tweets that discuss the article may not include a link, and the Twitter search API also has limitations that can lead to incomplete results. The number must therefore be seen as both incomplete and a lower bound. \n",
    "\n",
    "We can look more closely at data on the first tweet on the list. Bear in mind that the order of the list is not necessarily special. This is not the first tweet about this article chronologically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_plos.sources['twitter'].events[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use the twitter API to understand more about this person. For instance we could look at their twitter followers and followees or analyse the text of their tweets for topic modelling. Much work on social media interactions is done with this kind of data, using forms of network and text analysis described elsewhere in the book.\n",
    "\n",
    "A different approach is to integrate this data with information from another source. We might be interested for instance in whether the author of this tweet is a researcher, or whether they have authored research papers. One things we could do is search the ORCID API to see if there are any ORCID profiles that link to this Twitter handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitter_search = orcid.search(\"catmacOA\")\n",
    "\n",
    "for result in twitter_search:\n",
    "    print unicode(result)\n",
    "    print result.researcher_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the person with this Twitter handle seems to have an ORCID profile. That means we can also use ORCID to gather more information on their outputs. Perhaps they have authored work which is relevant to our article?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = orcid.get(\"0000-0001-9623-2225\")\n",
    "for pub in cm.publications[0:5]:\n",
    "    print pub.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this analysis we can show that this tweet is actually from one of the authors of the article.\n",
    "\n",
    "To make this process easier we can write a convenience function to go from a twitter user handle to try and find an ORCID for that person. There are a lot of ways this could be improved. One suggestion which is described in the Parameters of the function but not implemented is to change the function to return `True` or `False` so that the function could be used in a call to the `filter` python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def twitter2orcid(twitter_handle, twitter_users_name = None, resp = 'orcid', search_depth = 10):\n",
    "    \"\"\"\n",
    "    Take a twitter handle or user name and return an ORCID (or True/False for use as a filter)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    twitter_handle : str or unicode\n",
    "                     A twitter handle\n",
    "    twitter_users_name : str or unicode\n",
    "                         A twitter user's full name\n",
    "    resp : str\n",
    "           'orcid' or 'filter' If 'orcid' (default) return an ORCID if successful, else None, if 'filter' return \n",
    "           True/False for use as a filter function.\n",
    "    search_depth : int\n",
    "                   The number of returned results to test before giving up and declaring no match\n",
    "                   \n",
    "    Returns\n",
    "    -------\n",
    "    out : str or bool\n",
    "          Depending on the setting of resp returns either an ORCID (as returned by the ORCID search API) or True/False\n",
    "    \"\"\"\n",
    "    \n",
    "    search = orcid.search(twitter_handle)\n",
    "    s = [r for r in search]\n",
    "    orc = None\n",
    "    i = 0\n",
    "    while i < search_depth and orc == None and i < len(s):\n",
    "        arr = [('twitter.com' in website.url) for website in s[i].researcher_urls]\n",
    "        if True in arr:\n",
    "            index = arr.index(True)\n",
    "            url = s[i].researcher_urls[index].url\n",
    "            if url.lower().endswith(twitter_handle.lower()):\n",
    "                orc = s[i].orcid\n",
    "                return orc\n",
    "        \n",
    "        i+=1\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick test of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitter2orcid('catmacOA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a corpus\n",
    "\n",
    "In this case we will continue as previously to collect a set of works from a single ORCID profile. This collection could just as easily be a date range, or subject search at a range of other APIs. The target is to obtain a set of identifiers (in this case DOIs) that can be used to precisely query other data sources. This is a general pattern which reflects the issues of scope and source discussed above. The choice of how to construct a corpus to analyse will strongly affect the results and the conclusions that can be drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As previously, collect the set of DOIs available from an ORCID profile\n",
    "cn = orcid.get(\"0000-0002-0068-716X\")\n",
    "exids = []\n",
    "for pub in cn.publications:\n",
    "    if pub.external_ids:\n",
    "        exids = exids + pub.external_ids\n",
    "\n",
    "DOIs = [exid.id for exid in exids if exid.type == \"DOI\"]\n",
    "        \n",
    "len(DOIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have recovered 66 DOIs from the ORCID profile. Note that this isn't an identifier for every work (not all of them have DOIs). This illustrates an important point about data integration. In practice it is generally not worth the effort of attempting to integrate data on objects unless they have a unique identifier or key that can be used in multiple data sources. Hence the focus on DOIs and ORCIDs in these examples. Even in our search of the ORCID API for profiles that are associated with a Twitter account we used the Twitter handle as a unique ID to search on. \n",
    "\n",
    "While it is possible to work with names or titles of works and to disambiugate it is substantially more difficult. Other chapters deal with issues of data cleaning and disambiguation. Much work has been done on this basis but increasingly you will see that the first step in any analysis is to simply discard objects without a unique ID that can be used across data sources.\n",
    "\n",
    "We can obtain data for these from the DET API. As is common with many APIs there is a limit to how many queries can be simultaneously run, in this case 50, so we divide our query into batches.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches = [DOIs[0:50], DOIs[51:-1]]\n",
    "det_alms = []\n",
    "for batch in batches:\n",
    "    alms_response = pyalm.get_alm(batch, info=\"detail\", instance=\"det\")\n",
    "    det_alms.extend(alms_response.get('articles'))\n",
    "\n",
    "len(det_alms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DET API only provides information on a subset of Crossref DOIs. Data population has focussed on more recently published articles so only 24 responses are received in this case for the 66 DOIs we queried on. A good exercise would be to look at which of the DOIs are found and which are not. Let us see how much interesting data is available in the subset of DOIs for which we have data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for r in [d for d in det_alms if d.sources['wikipedia'].metrics.total != 0]:\n",
    "    print r.title\n",
    "    print '     ', r.sources['pmceurope'].metrics.total, 'pmceurope citations'\n",
    "    print '     ', r.sources['wikipedia'].metrics.total, 'wikipedia citations'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As discussed above this shows that the DET instance, while it provides information on a greater number of DOIs, has less complete data at this stage. Only four of the 24 responses have wikipedia references. You can change the code to look at the full set of 24 which shows very sparse data. The PLOS Lagotto instance provides more data but only on PLOS articles. However it does provide data on all the PLOS articles, going back earlier than the set returned by the DET instance. We can collect the set of articles from the profile published by PLOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plos_dois = []\n",
    "for doi in DOIs:\n",
    "    if doi.startswith('10.1371'): #This is quick and dirty, better would be to check Crossref API for publisher\n",
    "        plos_dois.append(doi)\n",
    "\n",
    "len(plos_dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plos_alms = pyalm.get_alm(plos_dois, info='detail', instance='plos').get('articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for article in plos_alms:\n",
    "    print article.title\n",
    "    print '     ', article.sources['crossref'].metrics.total, 'crossref citations'\n",
    "    print '     ', article.sources['twitter'].metrics.total, 'tweets'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous examples we know that we can obtain information on citing articles and tweets associated with this 66 articles. From that initial corpus we now have a collection of up to 86 related articles (cited and citing), a few hundred tweets that refer to (some of) those articles and perhaps 500 people if we include authors of both articles and tweets. Note how for each of these links our query is limited so we have a subset of all the related objects and agents. At this stage we probably have duplicate articles (one article might cite multiple in our set of seven) and duplicate people (authors in common between articles and authors who are also tweeting).\n",
    "\n",
    "This data could be used for network analysis, to build up a new corpus of articles (by following the citation) links or to analyse the links between authors and those tweeting about the articles. We won't pursue an in depth analysis here, but will gather the relevant objects and de-duplicate them as far as possible and count how many we have in preparation for future analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Collect all the citing DOIs and author names from citing articles\n",
    "citing_dois = []\n",
    "citing_authors = []\n",
    "for article in plos_alms:\n",
    "    for cite in article.sources['crossref'].events:\n",
    "        citing_dois.append(cite['event']['doi'])\n",
    "        citing_authors.extend(cite['event_csl']['author']) # Use 'extend' because the element is a list\n",
    "print '\\nBefore de-deduplication'\n",
    "print len(citing_dois), 'dois'\n",
    "print len(citing_authors), 'citing authors'\n",
    "\n",
    "\n",
    "# Easiest way to de-deplicate is to convert to a python set\n",
    "citing_dois = set(citing_dois)\n",
    "citing_authors = set([author['given'] + author['family'] for author in citing_authors])\n",
    "print '\\nAfter de-deduplication'\n",
    "print len(citing_dois), 'dois'\n",
    "print len(citing_authors), 'citing authors'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Collect all the tweets, usernames and check for any ORCIDs we can find.\n",
    "tweet_urls = set()\n",
    "twitter_handles = set()\n",
    "\n",
    "for article in plos_alms:\n",
    "    for tweet in article.sources['twitter'].events:\n",
    "        tweet_urls.add(tweet['event_url'])\n",
    "        twitter_handles.add(tweet['event']['user'])\n",
    "        \n",
    "# No need to explicitly de-duplicate because we created sets directly in this case\n",
    "print len(tweet_urls), 'tweets'\n",
    "print len(twitter_handles), 'twitter users'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be interesting to look at which twitter users interact most with the articles associated with this ORCID profile. To do that we would need to not create a set but a list and then count the number of duplicates in the list. The code could be easily modified to do this. Another useful exercise would be to search ORCID for profiles corresponding to citing authors. The best way to do this would be to obtain ORCIDS associated with each of the citing articles. However, because ORCID data is sparse and incomplete there are two limitations here. First that the author may not have an ORCID. Second that the article is not explicitly linked to article. Try searching ORCID for the DOIs associated with each of the citing articles.\n",
    "\n",
    "In this case we will look to see how many of the twitter handles discussing these articles are associated with an ORCID profile we can discover. This in turn could lead to more profiles and more cycles of analysis to build up a network of researchers interacting through citation and on twitter. Note we have inserted a delay between calls. This is because we are making a larger number of API calls (one for each Twitter handle). It is considered polite to keep the pace at which calls are made to an API to a reasonable level. The ORCID API does not post suggested limits at the moment but delaying for a second between calls is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_orcids = []\n",
    "for handle in twitter_handles:\n",
    "    orc = twitter2orcid(handle)\n",
    "    if orc:\n",
    "        tweet_orcids.append(orc)\n",
    "    time.sleep(1) # wait one second between each call to the ORCID API\n",
    "\n",
    "print len(tweet_orcids)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have identified twelve ORCID profiles we can positively link to tweets about this set of articles. This is a substantial under estimate of the likely number of ORCIDS associated with these tweets. However relatively few ORCIDs have twitter accounts registered as part of the profile. To gain a broader picture a search and matching strategy would need to be applied. Nonetheless for this eleven we can look closer into the profiles.\n",
    "\n",
    "The first step is to obtain the actual profile information for each of the twelve ORCIDs we have found. Note that at the moment what we have is the ORCIDs themselves, not the retrieved profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orcs = []\n",
    "for id in tweet_orcids:\n",
    "    orcs.append(orcid.get(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the profiles retrieved we can then take a look at who they are, and check that we do in fact have sensible twitter handles associated with them. We could use this to build up the network of related authors and Twitter users for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for orc in orcs:\n",
    "    i = [('twitter.com' in website.url) for website in orc.researcher_urls].index(True)\n",
    "    twitter_url = orc.researcher_urls[i].url\n",
    "    print orc.given_name, orc.family_name, orc.orcid, twitter_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
