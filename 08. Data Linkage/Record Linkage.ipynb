{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Linkage\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Initialization](#Initialization) \n",
    "- [Introduction](#Introduction)\n",
    "- [Data Definition](#Data-Definition)\n",
    "- [Parsing - with `split` method](#Parsing---with-split-method)\n",
    "- [Regular Expressions - RE or regex](#Regular-Expressions---RE-or-regex)\n",
    "\n",
    "    - [regex - Get Last Name](#regex---Get-Last-Name)\n",
    "    - [regex - Get First Name](#regex---Get-First-Name)\n",
    "    - [regex - Get Middle Name](#regex---Get-Middle-Name)\n",
    "    - [_Exercise 1 - Regular Expressions_](#Exercise-1---Regular-Expressions)\n",
    "\n",
    "- [String Comparators](#String-Comparators)\n",
    "- [Fellegi-Sunter Record Linkage](#Fellegi-Sunter-Record-Linkage)\n",
    "\n",
    "    - [_Exercise 2 - Value Comparison_](#Exercise-2---Value-Comparison)\n",
    "    - [_Exercise 3 - Record Comparison_](#Exercise-3---Record-Comparison)\n",
    "\n",
    "- [References & Further Readings](#References-&-Further-Readings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Before we start the Record linkage assignment, we need to import the packages we will be using.  Please run the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the modules required in this workbook\n",
    "import MySQLdb\n",
    "import numpy\n",
    "import pandas\n",
    "import re\n",
    "import jellyfish\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this lesson we will learn a little about how to use Python for cleaning input data, including using regular expressions, as well as the basic idea behind probabilistic record linkage. These two topics fit together naturally, because data cleaning can have a signficant impact on the success of record linkage. Being able to compare fields that were normalized the same way will give better results.\n",
    "\n",
    "We will use two datasets for the exercises in this chapter. The first will be a list of NIH projects and researchers pulled from the class database. The second is a list of university employees that was scraped from public web sites. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Definition\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Before we begin the task of record linkage, it's important that we understand the variables in our data. In this workbook, we will take a cursory look at some of the values in our data and compute some simple statistics to ensure that the content makes sense. \n",
    "\n",
    "Begin by loading the two data sets into pandas data frames. After we load the two data sets, we all the `head` method on the first data set to examine the first few reords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make connections to the database\n",
    "# Please use your own credentials\n",
    "user = \"jmorgan\"\n",
    "password = \"mCw7iLdQyV@TGeGon3fB\"\n",
    "database = \"homework\"\n",
    "\n",
    "# Invoke the connect() function, passing parameters in variables.\n",
    "db = MySQLdb.connect( user = user, passwd = password, db = database )\n",
    "\n",
    "# Output basic database connection info.\n",
    "print(db)\n",
    "\n",
    "# Load ucpay data directly through the DB connection\n",
    "uc = pandas.read_sql('select * from UCPay2011;', con = db)\n",
    "\n",
    "# Load nsf award data in 2010-2012 \n",
    "nsf = pandas.read_sql('select * from NSF_Award;', con = db)\n",
    "\n",
    "# Don't forget to close the connection\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the first few records from the UC employee file.\n",
    "uc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some initial thoughts about the data:\n",
    "* The ID field looks like a unique identifier that is specific to the data set. If we thought that we were going to use this identifier to link to other data from the same source, then we might take a loser look to see if the values should be interpreted as numbers or character strings. Ideally, this information would appear in the data documentation.\n",
    "* We should check to make sure that the year field contains valid values.\n",
    "* The campus and title fields look like they should be interpreted as finite categorical variables.\n",
    "* The name field appears to contain some redacted values, probably due to a privacy agreement. We will want to link the subset of valid name fields.\n",
    "\n",
    "Let's perform some quick summaries of the fields in the data.\n",
    "\n",
    "To get a list of the unique values in a pandas column/Series, call the `.unique()` method on it - like `.value_counts()` from last assignment, only not sorted by frequency of use.\n",
    "\n",
    "We'll ignore the numeric columns for now because we won't be using them for linkage, but a thorough data validation and definition process would ensure that these columns are valid and consistent as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the distinct values in the year, campus, and title columns\n",
    "print(\"Distinct years = \", uc[\"year\"].unique())\n",
    "print(\"Distinct campuses = \", uc[\"campus\"].unique())\n",
    "print(\"Distinct titles = \", uc[\"title\"].unique())\n",
    "\n",
    "# There are too many titles to display, so let's get the count\n",
    "print(\"Number of distinct titles = \", len(uc[\"title\"].unique()))\n",
    "\n",
    "# Print the total number of rows and the number of rows with valid name values\n",
    "print(\"Total rows = \", len(uc))\n",
    "name = uc[\"name\"]\n",
    "print(\"Rows with valid names = \", len(name[name != \"***********\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll take a look at the second data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nsf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* There are separate fields for first name and last name\n",
    "* The data include researchers from universities throughout the US, not just in the UC system\n",
    "* For UC schools we should be able to create a field corresponding to the campus field in the UC data, but this will require some recoding.\n",
    "\n",
    "In the next two sections we will address some of the data issues that we've identied. Before moving on, we will update the `uc` dataset so that it only contains records with valid name fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove those records without a valid name\n",
    "uc_name = uc[name != \"***********\"]\n",
    "uc_name.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing - with `split` method\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Above, we noted that we need to parse the name field in the UC data into first, middle, and last name fields. First, let's see what we can do with the built-in `split` method. By default, the `split` method returns a list of strings obtained by splitting the original string on spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Lorem Ipsum\".split())\n",
    "print(\"carrot cake\".split())\n",
    "print(\"Bogart, Humphrey\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `split` treats whitespace as delimiting characters, and multiple consecutive whitespace characters are treated like a single delimiter. We can also pass an argument to `split` to be the delimiter. If we set the delimiter explicitly, then multiple delimiters are not combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Hi,       Mom!\".split())\n",
    "print(\"Hi,       Mom\".split(\",\"))\n",
    "\n",
    "# in the following example, the result will include the empty strings between the first and second commas\n",
    "# and the second and third commas\n",
    "print(\"Hi,,,Mom!\".split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can map the `split` function over the values in the UC employee `name` column.  This uses `pandas` \"`.apply()`\" method to apply a function to all elements in a column.  To call a given function on a Series, pass its name to the \"`.apply()`\" function, including any package information needed (so in example below, we are telling it to call the \"`split`\" method from package \"`str`\"), not in quotation marks:\n",
    "\n",
    "    # get \"name\" column.\n",
    "    name_column_series = uc_name[ \"name\" ]\n",
    "\n",
    "    # apply the split method to each value in the column.\n",
    "    split_name_column_series = name_column_series.apply( str.split )\n",
    "\n",
    "More on \"`.apply()`\": [http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uc_name[\"name\"].apply(str.split).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty good! Fortunately for us, the UC data includes a space between the last name and the semicolumn separating it from the first name. This is unusual, but it means that the built-in `split` method is sufficient (we don't have to worry about parsing a semi-colon off each last name value).\n",
    "\n",
    "In the code cell that follows, we define functions `GetLastName` and `GetFirstName` that will call the `split` method and extract the appropriate values from the resulting list. We won't worry about middle names, because the NSF data doesn't include them.\n",
    "\n",
    "It turns out that all name fields in the `uc_name` data frame contain more than one word, even after filtering out the redacted names. Our `GetFirstName` function will have to perform a check to make sure that there is a first name to get.\n",
    "\n",
    "_**NOTE** - the following code displays a warning (essentially warning us that we are creating a new copy of a column - which we are!) that we can safely ignore._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simple functions that extract first name and last name\n",
    "def GetFirstName(name):\n",
    "    parts = name.split()\n",
    "    \n",
    "    if len(parts) >= 3:\n",
    "        return parts[2]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def GetLastName(name):\n",
    "    parts = name.split()\n",
    "    return parts[0]\n",
    "\n",
    "uc_name[\"FirstName\"] = uc_name[\"name\"].apply(GetFirstName)\n",
    "uc_name[\"LastName\"] = uc_name[\"name\"].apply(GetLastName)\n",
    "\n",
    "# Let's see the result\n",
    "uc_name[[\"name\", \"FirstName\", \"LastName\"]].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I intentionally showed 15 rows instead of the default 10 so that we could see an example of our simple approach failing. The second to last row in the result contains a last name that comprises several words. Our rule, which extracted the first word to be the \"last name\" does not give the correct result.\n",
    "\n",
    "There is a more subtle problem as well. It's likely that the third name, WOO YONG, should not be split into a first name and middle name component. That is, WOO YONG may in fact be the first name of the referenced individual. This format is common for names from some countries, including China and Korea.\n",
    "\n",
    "Let's redefine the `GetFirstName` and `GetLastName` functions so that the last name consists of everything before the semicolumn and the first name consists of the first word after the semicolumn. This doesn't solve our problem with Chinese and Korean names, but it's a good start.\n",
    "\n",
    "_**NOTE** - the following code also displays a warning (essentially warning us that we are creating a new copy of a column - which we are!) that we can safely ignore._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Improved functions that extract first name and last name\n",
    "def GetFirstName(name):\n",
    "    parts = name.split(\";\") \n",
    "    \n",
    "    if len(parts) > 1:\n",
    "        # The name contains a semicolumn. Take the part after the semicolumn, split on whitespace,\n",
    "        # and grab the first word\n",
    "        return parts[1].split()[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def GetLastName(name):\n",
    "    parts = name.split(\";\") \n",
    "    \n",
    "    # Return the first part (everything up to the first semicolumn)\n",
    "    # call the `strip` method to remove the space between the last name and the semicolumn\n",
    "    return parts[0].strip()\n",
    "\n",
    "uc_name[\"FirstName\"] = uc_name[\"name\"].apply(GetFirstName)\n",
    "uc_name[\"LastName\"] = uc_name[\"name\"].apply(GetLastName)\n",
    "\n",
    "# Let's see the result\n",
    "uc_name[[\"name\", \"FirstName\", \"LastName\"]].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions - RE or regex\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We were able to solve our parsing problem using the `split` method. Sometimes we need a more heavy duty way to search and extract text. In this section we introduce the basics of regular expressions, because they are a common approach to text parsing and a powerful one, too.\n",
    "\n",
    "Regular expressions are a mini-language for searching text. We have already noted that regular expressions are *powerful*, but they can also be *complex*. \n",
    "\n",
    "> Some people, when confronted with a problem, think \"I know, I'll use regular expressions.\" Now they have two problems. \n",
    "\n",
    "> &mdash; Jamie Zawinski\n",
    "\n",
    "### regex - Get Last Name\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In the previous section we were trying to extract first and last names from the UC name field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uc_name[\"name\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used basic python functions to try to split the names into first and last name components. In this section we'll use regular expressions to accomplish this task.\n",
    "\n",
    "When defining a regular expression search pattern, it is a good idea to start out by writing down, explicitly, in plain English, what you are trying to search for and exactly how you identify when you've found a match.\n",
    "\n",
    "For example, if we look at an author field formatted as \"`<last_name> ; <first_name> <middle_name>`\", in plain English, this is how I would explain where to find the last name: \"starting from the beginning of the line, take all the characters until you see a semi-colon.\"\n",
    "\n",
    "We can build a regular expression that captures this idea from the following components:\n",
    "\n",
    "* `^` Matches beginning of the line\n",
    "* `.` Matches any character\n",
    "* `+` A modifier that means \"match one or more of the preceding expression\"\n",
    "\n",
    "In a regular expression, there are special reserved characters and character classes like those in the list above.  Anything that is not a special character or class is just looked for explicitly (for example, a semi-colon is not a special character in regular expressions, so if it is in a regular expression pattern, the regular expression processor will just be looking for a semi-colon in the string, at that point in the pattern).\n",
    "\n",
    "This results in the regular expression:\n",
    "\n",
    "    ^.+;\n",
    "    \n",
    "We start at the beginning of the line ( \"^\" ), matching any characters ( \".+\" ) until we come to the literal character of a semi-colon ( \";\" ).\n",
    "\n",
    "In python, to use a regular expression like this to search for matches in a given string, we use the built-in \"`re`\" package ( [https://docs.python.org/2/library/re.html](https://docs.python.org/2/library/re.html) ), specifically the \"`re.search()`\" method.  To use \"`re.search()`\", pass it first the regular expression you want to use to search, enclosed in quotation marks, and then the string you want to search within:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_name_match = re.search(r\"^.+;\", \"Su ; Yu-Wen\")\n",
    "last_name_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things:\n",
    "* We used \"raw\" string syntax `r\"\"` to prevent python from escaping any special characters that might appear in our regular expression.\n",
    "* If the regular expression matches something in the searched string, then `m` holds a python `MatchObject` instance ( [https://docs.python.org/2/library/re.html#re.MatchObject](https://docs.python.org/2/library/re.html#re.MatchObject) ) or a child class of that object. Otherwise, it will be `None`.\n",
    "\n",
    "To see the matched substring, use the `group` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_name_match.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is almost what we want, but not quite.  We need the semi-colon in our regular expression in order to know where the last name ends, but it would be nice if we could omit the semi-colon from the value we find.  The solution is to use regular expression \"groups\".  Within a regular expression, one creates a \"group\" by putting parentheses around a part of the regular expression that one wants to refer to later.  You can have mulitple \"groups\" in a single regular expression.\n",
    "\n",
    "* `(...)` Creates a group that can be referred to later.\n",
    "\n",
    "So, in our regular expression, we'll place any characters up to a comma in parentheses: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run regular expression\n",
    "last_name_match = re.search(r\"^(.+);\", \"Franklin ; Benjamin\")\n",
    "\n",
    "# get full match text\n",
    "full_match_text = last_name_match.group()\n",
    "\n",
    "# get contents of first group defined in regex.\n",
    "first_group_match_text = last_name_match.group( 1 )\n",
    "\n",
    "# print results\n",
    "print( \"- Full match text: \\\"\" + full_match_text + \"\\\"\" )\n",
    "print( \"- First group match text: \\\"\" + first_group_match_text + \"\\\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a match-data object like `last_name_match`:\n",
    "\n",
    "- `last_name_match.group()` is a synonym for `last_name_match.group(0)`.  Either of these refers to what was matched by the entire regular expression, not just the matches contained in an individual group.\n",
    "- `last_name_match.group(1)` will show you what was matched by the expression between the first set of parentheses.\n",
    "- `last_name_match.group(2)` will show you what was matched by the expression between the second set of parentheses, if a second group was defined.\n",
    "- ... - and so on, for as many groups as you define.\n",
    "\n",
    "### regex - Get First Name\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now let's write a regular expression that will match the first name of the author (that is, the first word after the semi-colon). In addition to the bits of regular expressions we saw above, here are some other useful character classes:\n",
    "\n",
    "* `\\w` Matches a single alphanumeric character\n",
    "* `\\W` Matches anything that is not an alphanumeric character\n",
    "* `\\s` Matches a single whitespace character\n",
    "* `\\S` Matches anything that is not a whitespace character.\n",
    "* `*` Matches 0 or more instances of the expresson that precedes the asterisk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = r\";\\s*(\\w+)\"\n",
    "m = re.search(regex, \"Eisenhower ; Dwight David\")\n",
    "m.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above regular expression:\n",
    "\n",
    "    ;\\s*(\\w+)\n",
    "    \n",
    "matches as follows\n",
    "\n",
    "- first finds a semi-colon - \"`;`\".\n",
    "- then, after the semi-colon, the \"`\\s*`\" looks for from 0 to as many as are embedded white space characters immediately following the semi-colon character.\n",
    "- the \"`(\\w+)`\" group definition then matches any 1 or more alphanumeric charaters - \"`\\w+`\", and stores that set of characters in a group.\n",
    "\n",
    "Regular expressions are a powerful tool and we're barely scratching the surface. (See - [References & Further Reading](#References-&-Further-Readings) for more information)\n",
    "\n",
    "### regex - Get Middle Name\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now is the time to practise what we have learned (parsing strings and regular expression in Python). As we already have walked through the example of extracting first name and last name, we should be able to get middle name as well. The following is an example how we can do this by using python parsing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this cell, you will see the example function using split and join to obtain middle names\n",
    "# Note that the middle names appear after comma and come after first name if they exist.\n",
    "\n",
    "def GetMiddleName_split(name):\n",
    "    '''\n",
    "    name: given a person's full name\n",
    "    '''\n",
    "    # extract the middle name based on its position in the full name\n",
    "    # we can use \"join\" to unlist in python.\n",
    "\n",
    "    parts = name.split(\";\")\n",
    "    \n",
    "    if len(parts[1]) > 1:\n",
    "        # The name contains a semicolumn. Take the part after the semicolumn, split on whitespace,\n",
    "        # and grab starting from the second word\n",
    "\n",
    "        # extract the middle name based on its position in the full name\n",
    "        # We can use \"join\" to unlist in python.\n",
    "        if \" \".join(parts[1].split()[1:]):\n",
    "            return \" \".join(parts[1].split()[1:])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Regular Expressions\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In the exercise, you will be asked to complete a function that uses regular expressions to parse the middle name out of an author string.\n",
    "\n",
    "Notes on this exercise:\n",
    "\n",
    "- for this exercise, we define a \"middle name\" as the part of a name string that:\n",
    "\n",
    "    - is to the right of the semi-colon\n",
    "    - AND to the right of the first name\n",
    "    - AND to the right of any whitespace that follows the first name\n",
    "    \n",
    "- It is a good idea to test your regular expression before you run it in your function.  You can test a regular expression online at [https://regex101.com/#python](https://regex101.com/#python).  Enter your regular expression string, without surrounding quotation marks, in the \"REGULAR EXPRESSION\" field at the top of the page, then enter a test name string in the \"TEST STRING\" box just below it.\n",
    "- You can (and should!) use one or more groups - () - to target just the middle name within your larger regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "EX1-RE-middle_name",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# In this cell, you need to define your own function using regular expression to obtain middle names\n",
    "# Note that the middle names appear after semicolumn and come after first name and a whitespace.\n",
    "# Please be careful when you define your own regular expression.\n",
    "# You can use either group() or groups() to get middle names in the last step.\n",
    "\n",
    "\n",
    "def GetMiddleName_re(name):\n",
    "    \n",
    "    '''\n",
    "    name: given a person's full name\n",
    "    '''\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    # Define the regular expression to get the middle name\n",
    "    regex = r\";\\s*\\w+\\s(\\w+)\"\n",
    "    # Store the re.search outcome\n",
    "    pattern = re.search(regex, name)\n",
    "    # We can use groups() to get the matched middle names\n",
    "    # If Null, the function should return None.\n",
    "    if pattern:\n",
    "        return pattern.groups()[0]\n",
    "    else:\n",
    "        return None\n",
    "    ### END SOLUTION\n",
    "    \n",
    "#-- END function GetMiddleName_re() --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we've provided two code cells to help you assess how well your \"`GetMiddleName_re()`\" function compares to the middle name finder defined above ( \"`GetMiddleName_split()`\" ).\n",
    "\n",
    "In the first cell, we populate columns/Series with the results of each method for each row:\n",
    "\n",
    "- 'MiddleName' contains result of \"`GetMiddleName_split()`\"\n",
    "- 'MiddleName_re' contains result of \"`GetMiddleName_re()`\"\n",
    "- BONUS - 'MiddleNameMatch' contains a 1 if the value of 'MiddleName' is equal to the value in 'MiddleName_re', and a 0 if not.\n",
    "\n",
    "We then display the first 30 rows, so you can visually compare the middle names found by the two different methods.\n",
    "\n",
    "_**NOTE** - the following code also displays warnings that we can safely ignore._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, Let's see how your functions work\n",
    "\n",
    "# place split-based middle name in \"MiddleName\"\n",
    "uc_name[\"MiddleName\"] = uc_name[\"name\"].apply(GetMiddleName_split)\n",
    "\n",
    "# place regular expression-based middle name in \"MiddleName_re\"\n",
    "uc_name[\"MiddleName_re\"] = uc_name[\"name\"].apply(GetMiddleName_re)\n",
    "\n",
    "# create column \"MiddleNameMatch\" where value is 1 of two middle names match, 0 if not.\n",
    "uc_name[ \"MiddleNameMatch\" ] = numpy.where( uc_name[\"MiddleName\"].isin( uc_name[\"MiddleName_re\"] ), 1, 0 )\n",
    "\n",
    "# output first 30 columns.\n",
    "uc_name[ [ \"name\", \"FirstName\", \"LastName\",\"MiddleName\",\"MiddleName_re\", \"MiddleNameMatch\" ] ].head( 30 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second, we then divide the sum of the values in the 'MiddleNameMatch' column (the count of matches) by the total number of rows and multiply the result by 100 to get percentage agreement.\n",
    "\n",
    "Your goal should be to get 80% agreement or better (for as simple as it is to parse name parts for a person, this is a relatively tough problem in computer science)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "EX1-RE-middle_name-test",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST - calculate percent agreement to see how well the two columns match.\n",
    "\n",
    "# get sum of new column\n",
    "match_count = uc_name[ \"MiddleNameMatch\" ].sum()\n",
    "\n",
    "# get length of DataFrame\n",
    "row_count = len( uc_name )\n",
    "\n",
    "# calculate percentage agreement\n",
    "from __future__ import division\n",
    "percent_agree = ( match_count / row_count ) * 100\n",
    "\n",
    "print( \"Percent agreement = \" + str( percent_agree ) )\n",
    "assert percent_agree > 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Comparators\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this section we will demonstrate different string comparison algorithms provided by the [jellyfish](https://github.com/sunlightlabs/jellyfish) package ( [https://github.com/sunlightlabs/jellyfish](https://github.com/sunlightlabs/jellyfish) ).\n",
    "\n",
    "For each method we examine, we'll write a function that accepts a name that we want to find matches for in the NSF data and that returns a list of the names in the NSF data that are most similar to the name of interest.\n",
    "\n",
    "We will start by creating a `set` of unique first names from the NSF data. The `FirstName` field is missing some values which are represented as NaN in the data frame. To prevent errors later on, we only include valid character strings (which have type `str`) in our list of unique names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If we were starting from scratch, we'd need to import jellyfish\n",
    "# import jellyfish\n",
    "\n",
    "# Make a set storing the unique first name with respect to the nsf dataset\n",
    "unique_first_names = set( name for name in nsf[ \"FirstName\" ] if type( name ) == str )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define our function \"`closest_names`\" that:\n",
    "\n",
    "* Accepts a string name we are interested in matching with NSF names as input argument \"`name_IN`\".\n",
    "* Accepts an optional number of results we want returned as input argument \"`result_count_IN`\".\n",
    "* Compares the name in `name_IN` to each name in `unique_first_names` and calculates the \"distance\" between the two strings using the Levenshtein Distance string comparator from `jellyfish`.\n",
    "* Return a list of size `result_count_IN` of names in `uniq_first_names` that are \"closest\" to `name_IN`.\n",
    "\n",
    "From wikipedia, the Levenshtein Distance is defined as:\n",
    "\n",
    "> \"In information theory and computer science, the Levenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (i.e. insertions, deletions or substitutions) required to change one word into the other. It is named after Vladimir Levenshtein, who considered this distance in 1965.\"\n",
    "\n",
    "> &mdash; [https://en.wikipedia.org/wiki/Levenshtein_distance](https://en.wikipedia.org/wiki/Levenshtein_distance)\n",
    "\n",
    "_Note that in the comparison we capitalize both names being compared, so that letter case doesn't affect the final distance._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def closest_names( name_IN, result_count_IN = 10 ):\n",
    "\n",
    "    # return reference\n",
    "    results_OUT = []\n",
    "    \n",
    "    # declare variables\n",
    "    other_name = \"\"\n",
    "    cleaned_name = \"\"\n",
    "    cleaned_other_name = \"\"\n",
    "    get_distance_lambda = None\n",
    "    \n",
    "    # first, standardize the name - convert to upper case and to unicode.\n",
    "    cleaned_name = name_IN.upper()\n",
    "    cleaned_name = unicode( cleaned_name )\n",
    "    \n",
    "    # First create a list of tuples (other_name, distance), where other_name is taken from uniq_first_names\n",
    "    distances = []\n",
    "\n",
    "    # loop over unique_first_names to calculate and store distances\n",
    "    for other_name in unique_first_names:\n",
    "        \n",
    "        # standardize the other name.\n",
    "        cleaned_other_name = other_name.upper()\n",
    "        cleaned_other_name = unicode( cleaned_other_name )\n",
    "        \n",
    "        # get distance from name to other_name (converted to upper case so we are case-insensitive.)\n",
    "        distance_value = jellyfish.levenshtein_distance( cleaned_name, cleaned_other_name )\n",
    "        \n",
    "        # add tuple to distances\n",
    "        current_tuple = ( other_name, distance_value )\n",
    "        distances.append( current_tuple )\n",
    "    \n",
    "    #-- END loop over unique_first_names --#\n",
    "    \n",
    "    # Sort distances by the second element in the tuple.\n",
    "    \n",
    "    # define lambda function to retrieve the distance (the second item in the tuple)\n",
    "    #    and return it.  Lambda functions are little one line functions.  More information:\n",
    "    #    https://docs.python.org/2/reference/expressions.html#lambda\n",
    "    get_distance_lambda = lambda distance_tuple_list_IN : distance_tuple_list_IN[ 1 ]\n",
    "    \n",
    "    # sort matching names by distance\n",
    "    results_OUT = sorted( distances, key = get_distance_lambda )\n",
    "    \n",
    "    # get the number of results requested using Python's slice notation.\n",
    "    results_OUT = results_OUT[ : result_count_IN ]\n",
    "    \n",
    "    # return results\n",
    "    return results_OUT\n",
    "    \n",
    "    '''\n",
    "    # For reference, compacted version - you can do this, but please don't.\n",
    "    \n",
    "    # First create a list of tuples (other_name, distance), where other_name is taken from uniq_first_names\n",
    "    distances = [ ( other_name, jellyfish.levenshtein_distance( unicode( name_IN.upper() ), unicode( other_name.upper() ) ) )\n",
    "        for other_name in unique_first_names ]\n",
    "\n",
    "    # Sort distances by the second element in the tuple, and return the top n values\n",
    "    return sorted(distances, key=lambda x: x[1])[:result_count_IN]\n",
    "    '''\n",
    "\n",
    "#-- END function closest_names() --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out on some names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Experiment the function with several names.\n",
    "print( closest_names( \"Jennifer\" ) )\n",
    "print( closest_names( \"Sonya\" ) )\n",
    "print( closest_names( \"Wai Tong\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that Levenshtein distance is a kind of edit distance. Edit distances count the number of edit operations needed to change one word to another, and different edit distances count different edit operations as valid. In the case of Levenshtein distance, the valid edit operations are inserting a letter, deleting a letter, or changing a letter. \n",
    "\n",
    "It would be interesting to compare this output to the output from other string comparators included in the jellyfish package:\n",
    "\n",
    "* **`jellyfish.lenvenstein_distance`** - _Levenshtein distance_: edit distance where the valid operations are inserting a letter, deleting a letter, or changing a letter\n",
    "* **`jellyfish.damerau_levenshtein_distance`** - _Levenshtein-Damerau distance_: edit distance which includes the same operations as Levenshtein distance but also allows transposing two adjacent letters. This can be useful for finding words with typos.\n",
    "* **`jellyfish.jaro_winkler`** - _Jaro-Winkler distance_: a fast-to-compute string distance based on common letters between two words\n",
    "\n",
    "_Note: For edit distance smaller numbers indicate closer strings, but for Jaro-Winkler distance larger values indicate closer strings._\n",
    "\n",
    "Let's update our `closest_names` function so that we can specify the string comparator we want to use.  Changes from previous function:\n",
    "\n",
    "- add ability to pass in the string distance calculation function you want to use as an argument, named \"`string_comparator_function_IN`\".\n",
    "\n",
    "    - just pass the name of the function, not in quotation marks, and not followed by parentheses (just like they are shown in the list of functions above).\n",
    "\n",
    "- add ability to reverse sort order for returning \"closest\" strings to name passed in.  New parameter \"`reverse_sort_IN`\" defaults to `False` (to match distance scores where a larger number indicates two strings being further apart).  Set it to `True` for distance scores like Jaro-Winkler distance where a larger number indicates two strings are closer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def closest_names_2( string_comparator_function_IN, name_IN, reverse_sort_IN = False, result_count_IN = 10 ):\n",
    "\n",
    "    # return reference\n",
    "    results_OUT = []\n",
    "    \n",
    "    # declare variables\n",
    "    other_name = \"\"\n",
    "    cleaned_name = \"\"\n",
    "    cleaned_other_name = \"\"\n",
    "    get_distance_lambda = None\n",
    "    \n",
    "    # first, standardize the name - convert to upper case and to unicode.\n",
    "    cleaned_name = name_IN.upper()\n",
    "    cleaned_name = unicode( cleaned_name )\n",
    "    \n",
    "    # First create a list of tuples (other_name, distance), where other_name is taken from uniq_first_names\n",
    "    distances = []\n",
    "\n",
    "    # loop over unique_first_names to calculate and store distances\n",
    "    for other_name in unique_first_names:\n",
    "        \n",
    "        # standardize the other name.\n",
    "        cleaned_other_name = other_name.upper()\n",
    "        cleaned_other_name = unicode( cleaned_other_name )\n",
    "        \n",
    "        # get distance from name to other_name (converted to upper case so we are case-insensitive.)\n",
    "        distance_value = string_comparator_function_IN( cleaned_name, cleaned_other_name )\n",
    "        \n",
    "        # add tuple to distances\n",
    "        current_tuple = ( other_name, distance_value )\n",
    "        distances.append( current_tuple )\n",
    "    \n",
    "    #-- END loop over unique_first_names --#\n",
    "    \n",
    "    # Sort distances by the second element in the tuple.\n",
    "    \n",
    "    # define lambda function to retrieve the distance (the second item in the tuple)\n",
    "    #    and return it.  Lambda functions are little one line functions.  More information:\n",
    "    #    https://docs.python.org/2/reference/expressions.html#lambda\n",
    "    get_distance_lambda = lambda distance_tuple_list_IN : distance_tuple_list_IN[ 1 ]\n",
    "    \n",
    "    # sort matching names by distance\n",
    "    results_OUT = sorted( distances, key = get_distance_lambda, reverse = reverse_sort_IN )\n",
    "    \n",
    "    # get the number of results requested using Python's slice notation.\n",
    "    results_OUT = results_OUT[ : result_count_IN ]\n",
    "    \n",
    "    # return results\n",
    "    return results_OUT\n",
    "    \n",
    "    '''\n",
    "    # For reference, compacted version - you can do this, but please don't.\n",
    "    \n",
    "    # First create a list of tuples (other_name, distance), where other_name is taken from uniq_first_names\n",
    "    distances = [(other_name, string_comparator(unicode(name.upper()), unicode(other_name.upper())))\n",
    "                 for other_name in uniq_first_names]\n",
    "    \n",
    "    # Sort distances by the second element in the tuple, and return the top n values\n",
    "    return sorted(distances, key=lambda x: x[1], reverse=reverse_sort)[:n]\n",
    "    '''\n",
    "\n",
    "#-- END function closest_names_2() --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try it!\n",
    "print( \"Closest names for \\\"William\\\" using Levenshtein-Damerau distance:\" )\n",
    "print( closest_names_2( jellyfish.damerau_levenshtein_distance, \"William\" ) )\n",
    "\n",
    "print( \"\\n\\nClosest names for \\\"William\\\" using Levenshtein-Damerau distance:\" )\n",
    "print( closest_names_2( jellyfish.jaro_winkler, \"Wiliam\", reverse_sort_IN = True ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fellegi-Sunter Record Linkage\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fellegi-Sunter Record Linkage is a probabilistic method that uses comparisons of fields that contain the same substantive types of data between records to calculate a weighted probability that records in different data sets refer to the same entity.  Examples of the types of data items that might be compared in this method include gender, date of birth, age, and parts of a name.\n",
    "\n",
    "In this section we will \"manually\" perform the steps in Fellegi-Sunter record linkage. Our goal is to illustrate the Fellegi-Sunter algorithm by breaking it into bitesize pieces. \n",
    "\n",
    "In our example we will compare first names and last names using Jaro-Winkler distance. In the Fellegi-Sunter algorithm, the result of a field comparison is assumed to follow a multinomial distribution. That means it can only take on finitely many values. Therefore we will define a function that compares two strings and returns the value 2, 1, or 0 to indicate an exact match, a nearly exact match, or anything else. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Value Comparison\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In exercise 2, we will implement the value comparison stage of Fellegi-Sunter Record Linkage.  You will implement a function named \"`fuzzy-string-comparator`\" that accepts two strings and returns one of the following match levels:\n",
    "\n",
    "- 2 - exact match\n",
    "- 1 - close match\n",
    "- 0 - not a match\n",
    "\n",
    "To assess whether the two strings passed in match, we'll convert both strings to capital letters, decode them into unicode, then calculate the Jaro-Winkler distance between the two strings.  We'll then assign a match level based on where the resulting match score falls in the following ranges:\n",
    "\n",
    "- 2 - exact match - score greater than or equal to ( >= ) 0.92\n",
    "- 1 - close match - score less than 0.92 but greater than or equal to 0.85.\n",
    "- 0 - not a match - score less than 0.85.\n",
    "\n",
    "Finally, we'll return that match score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "EX2-string_compare",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Please complete the following function that tells us how different two input strings are.\n",
    "# It returns a match level with value 2, 1 or 0 (larger value means higher similarity)\n",
    "# Calculate Jaro-Winkler distance after converting two strings into capital characters.\n",
    "# Please use these three criteria, >=0.92, >=0.85, <0.85, to determine match level.\n",
    "\n",
    "def fuzzy_string_comparator( string_1_IN, string_2_IN ):\n",
    "\n",
    "    '''\n",
    "    string_1_IN : input string No.1\n",
    "    string_2_IN : input string No.2\n",
    "    '''\n",
    "    \n",
    "    # return reference\n",
    "    match_level_OUT = -1\n",
    "\n",
    "    # Check if they are all strings\n",
    "    if ( ( type( string_1_IN ) != str ) or ( type( string_2_IN ) != str ) ):\n",
    "        \n",
    "        match_level_OUT = 0\n",
    "    \n",
    "    #-- END check to see if strings are actually strings. --#\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    # declare variables\n",
    "    cleaned_string_1 = \"\"\n",
    "    cleaned_string_2 = \"\"\n",
    "    distance = -1\n",
    "    \n",
    "    # convert strings to upper case, then to unicode\n",
    "    \n",
    "    # string 1\n",
    "    cleaned_string_1 = string_1_IN.upper()\n",
    "    cleaned_string_1 = unicode( cleaned_string_1 )\n",
    "\n",
    "    # string 2\n",
    "    cleaned_string_2 = string_2_IN.upper()\n",
    "    cleaned_string_2 = unicode( cleaned_string_2 )\n",
    "    \n",
    "    # Calculate Jaro-Winkler distance after converting two strings into capital characters.\n",
    "    distance = jellyfish.jaro_winkler( cleaned_string_1, cleaned_string_2 )\n",
    "\n",
    "    # According to different thresholds, return the match level\n",
    "    if distance >= 0.92:\n",
    "\n",
    "        match_level_OUT = 2\n",
    "\n",
    "    elif distance >= 0.85:\n",
    "    \n",
    "        match_level_OUT = 1\n",
    "\n",
    "    else:\n",
    "    \n",
    "        match_level_OUT = 0\n",
    "        \n",
    "    #-- END conditional to set match level. --#\n",
    "\n",
    "    ### END SOLUTION\n",
    "\n",
    "    return match_level_OUT\n",
    "\n",
    "#-- END function fuzzy_string_comparitor --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "EX2-string_compare-test",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's see how the fuzzy_string_comparator works\n",
    "score1 = fuzzy_string_comparator( \"joshua\", \"joshua\" )\n",
    "score2 = fuzzy_string_comparator( \"joshua\", \"joshau\" )\n",
    "score3 = fuzzy_string_comparator( \"joshua\", \"todd\" )\n",
    "\n",
    "print( \"Match level for joshua-joshua: \" + str( score1 ) )\n",
    "print( \"Match level for joshua-joshau: \" + str( score2 ) )\n",
    "print( \"Match level for joshua-todd: \" + str( score3 ) )\n",
    "\n",
    "# tests for our grading program:\n",
    "assert score1 == 2\n",
    "assert score2 == 2\n",
    "assert score3 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function compares *text fields* in a record (other types of data would need different means of comparison). Next, we define a function that compares *records*.  This record comparison function assumes that records will have the form of a tuple: (identifier, first name, last name).  It returns a length 2 tuple that gives the result of applying our fuzzy string comparator to the first name and to the last name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Comparison_vector compare a pair of records, which consists of first name and last name.\n",
    "# It returns a tuple with 2 match levels.\n",
    "\n",
    "def compare_records( record_1_IN, record_2_IN ):\n",
    "\n",
    "    '''\n",
    "    record_1_IN : input record No.1\n",
    "    record_2_IN : input record No.2\n",
    "    '''\n",
    "    \n",
    "    # return reference\n",
    "    results_OUT = None\n",
    "    \n",
    "    # declare variables\n",
    "    field_1_match_level = -1\n",
    "    field_2_match_level = -1\n",
    "    \n",
    "    # record_1_IN and record_2_IN have the form (id, first name, last name)\n",
    "    \n",
    "    # m, n store the comparing outcomes of first name and last name.\n",
    "    field_1_match_level = fuzzy_string_comparator( record_1_IN[ 1 ], record_2_IN[ 1 ] )\n",
    "    field_2_match_level = fuzzy_string_comparator( record_1_IN[ 2 ], record_2_IN[ 2 ] )\n",
    "\n",
    "    results_OUT = ( field_1_match_level, field_2_match_level )\n",
    "\n",
    "    return results_OUT\n",
    "\n",
    "#-- END function compare_records() --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try out the compare_records function\n",
    "\n",
    "print( compare_records( ( 1, \"joshua\", \"tokle\" ), ( 2, \"joshua\", \"smith\") ) )\n",
    "print( compare_records( ( 3, \"joshua\", \"tokle\" ), ( 4, \"josue\", \"tolke\") ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Record Comparison\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Next, we'll work on implementing the section of Fellegi-Sunter Record Linkage that calculates a weighted probability that two records from different data sets refer to the same entity.\n",
    "\n",
    "Fellegi-Sunter Record Linkage uses two different sets of probabilities per pair of data items as weights in this step: m-weights and u-weights.\n",
    "\n",
    "For a given pair of data items that represent the same conceptual thing, for each match level:\n",
    "- An **m-weight** is the probability of seeing a particular match level if we assume that we are comparing two records that represent the same individual.\n",
    "- A **u-weight** is the probability of seeing a particular match level if we assume that that we are comparing two records that do *not* represent the same individual.\n",
    "\n",
    "For example, thinking of probabilities when two records are the same (m-weights), if two records represent the same person, the first names and last names should match with high probability - match level 2.  So, the m-weight for first name and last name having a match level of 2 when two records refer to the same person should be large.\n",
    "\n",
    "On the other hand, in the context of probabilities when two records are different (u-weights), suppose we had month of birth in our data set.  The probability that two random individuals will have the same month of birth is about 1/12, so for records that are not the same person, we would assign a u-weight of about 1/12 to the birth year being identical (where for a field like social security number, the u-weight of two different people having the same social security number is 0).\n",
    "\n",
    "Let's assign some preliminary and arbitrary m- and u-weights for first name and last name.\n",
    "\n",
    "- first name\n",
    "\n",
    "    - m-weights:\n",
    "\n",
    "        - match level 0: **_0.01_** (very unlikely the same person will have different first names)\n",
    "        - match level 1: **_0.14_** (also pretty unlikely that first names for a person will be mostly the same)\n",
    "        - match level 2: **_0.85_** (very likely that a person's first names will match exactly)\n",
    "\n",
    "    - u-weights:\n",
    "\n",
    "        - match level 0: **_0.88_** (probability that different people will have different first names)\n",
    "        - match level 1: **_0.10_** (probability that different people's first names will be mostly the same)\n",
    "        - match level 2: **_0.02_** (probability that different people will have same first name)\n",
    "- last name\n",
    "\n",
    "    - m-weights:\n",
    "\n",
    "        - match level 0: **_0.01_** (very unlikely the same person will have different last names)\n",
    "        - match level 1: **_0.09_** (also pretty unlikely that last names for a person will be mostly the same)\n",
    "        - match level 2: **_0.90_** (very likely that a person's last names will match exactly)\n",
    "\n",
    "    - u-weights:\n",
    "\n",
    "        - match level 0: **_0.91_** (probability that different people will have different first names)\n",
    "        - match level 1: **_0.08_** (probability that different people's first names will be mostly the same)\n",
    "        - match level 2: **_0.01_** (probability that different people will have same first name\n",
    "    \n",
    "In practice you would likely start with guesses or very general estimates like these, but then try to better estimate these by trying to fit them to a model or at least tweaking them after seeing preliminary output. In this case, we are just going to run with our initial estimates so we can move efficiently through this process.\n",
    "\n",
    "In the cell below, we create dictionaries that contain the m- and u-weights for our two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make dictionaries to hold m_weights and u-weights.  In each dictionary, the weights for\n",
    "#    a given field are mapped to a string label for that field (\"first_name\" and \"last_name\").\n",
    "# Weights are captured in lists of length 3, with the index in the list matching each of the \n",
    "#   match levels that can be returned by the fuzzy string comparator.\n",
    "# In this list, we go from match level 0 (not the same), to match level 2 (identical)\n",
    "#    as we move from left to right in the list, with each position in the list holding the\n",
    "#    corresponding weight for that match level.\n",
    "\n",
    "m_weights_dict = {}\n",
    "m_weights_dict[ \"first_name\" ] = [ 0.01, 0.14, 0.85 ]  # m-weights corresponding to first name\n",
    "m_weights_dict[ \"last_name\" ] = [ 0.01, 0.09, 0.90 ] # m-weights corresponding to last name\n",
    "\n",
    "u_weights_dict = {}\n",
    "u_weights_dict[ \"first_name\" ] = [ 0.88, 0.10, 0.02 ] # u-weights corresponding to first name\n",
    "u_weights_dict[ \"last_name\" ] = [ 0.91, 0.08, 0.01 ] # u-weights corresponding to last name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for exercise 3, we are going to define a function that uses these weights to compare two records and return their record-level match score.\n",
    "\n",
    "In Fellegi-Sunter Record Linkage the match score for a given record is the at calculates a weighted probability that two records from different data sets refer to the same entity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def match_score( record_1_IN, record_2_IN ):\n",
    "\n",
    "    '''\n",
    "    record_1_IN: input record No.1\n",
    "    record_2_IN: input record No.2\n",
    "    '''\n",
    "    \n",
    "    # return reference\n",
    "    score_OUT = -1\n",
    "    \n",
    "    # declare variables\n",
    "    match_level_tuple = None\n",
    "    match_level_first_name = -1\n",
    "    match_level_last_name = -1\n",
    "    \n",
    "    # Calulate the similarity level using compare_records\n",
    "    match_level_tuple = compare_records( record_1_IN, record_2_IN )\n",
    "    match_level_first_name = match_level_tuple[ 0 ]\n",
    "    match_level_last_name = match_level_tuple[ 1 ]\n",
    "    \n",
    "    # Use match levels and m- and u-weights to calculate a match score for this record.\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    # declare variables\n",
    "    m_weights_list_first_name = None\n",
    "    m_weights_list_last_name = None\n",
    "    u_weights_list_first_name = None\n",
    "    u_weights_list_last_name = None\n",
    "    first_name_m_weight = -1\n",
    "    first_name_u_weight = -1\n",
    "    last_name_m_weight = -1\n",
    "    last_name_u_weight = -1\n",
    "    log_prob_given_match = None\n",
    "    log_prob_given_nonmatch = None\n",
    "    \n",
    "    # get lists of m- and u- weights for each field from weights dictionaries defined above.\n",
    "    m_weights_list_first_name = m_weights_dict[ \"first_name\" ]\n",
    "    m_weights_list_last_name = m_weights_dict[ \"last_name\" ]\n",
    "    u_weights_list_first_name = u_weights_dict[ \"first_name\" ]\n",
    "    u_weights_list_last_name = u_weights_dict[ \"last_name\" ]\n",
    "   \n",
    "    # get weights for match levels returned by compare_records.\n",
    "    first_name_m_weight = m_weights_list_first_name[ match_level_first_name ]\n",
    "    first_name_u_weight = u_weights_list_first_name[ match_level_first_name ]\n",
    "    last_name_m_weight = m_weights_list_last_name[ match_level_last_name ]\n",
    "    last_name_u_weight = u_weights_list_last_name[ match_level_last_name ]\n",
    "    \n",
    "    # calculate log-probabilities for each field assuming a match (m-weight),\n",
    "    #    and assuming a non-match (u-weight).  Log-probability is the natural\n",
    "    #    log (math.log() in Python) of the probability of a given match level.\n",
    "    log_prob_first_name_given_match = math.log( first_name_m_weight )\n",
    "    log_prob_last_name_given_match = math.log( last_name_m_weight )\n",
    "    log_prob_first_name_given_no_match = math.log( first_name_u_weight )\n",
    "    log_prob_last_name_given_no_match = math.log( last_name_u_weight )\n",
    "    \n",
    "    # For match and no-match, sum the log-probabilities for each field.\n",
    "    \n",
    "    # What's the log-probability of seeing this comparison vector if the records are a match?\n",
    "    log_prob_given_match = log_prob_first_name_given_match + log_prob_last_name_given_match\n",
    "    \n",
    "    # What's the log-probability of seeing this comparison vector if the records are a nonmatch?\n",
    "    log_prob_given_nonmatch = log_prob_first_name_given_no_match + log_prob_last_name_given_no_match\n",
    "    \n",
    "    # match score is the sum of the log probabilities given a match\n",
    "    #    minus the sum of the log probabilites given no match.\n",
    "    score_OUT = log_prob_given_match - log_prob_given_nonmatch\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    \n",
    "    # return match score.\n",
    "    return score_OUT\n",
    "\n",
    "#-- END function match_score() --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Have a rough look at its sample output\n",
    "print(match_score((1, \"joshua\", \"tokle\"), (2, \"joshua\", \"smith\")))\n",
    "print(match_score((1, \"joshua\", \"tokle\"), (4, \"joshu\", \"tolke\")))\n",
    "print(match_score((1, \"joshua\", \"tokle\"), (7, \"christina\", \"jones\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can try to link UC employees to NSF researchers. We will apply **blocking** to reduce the number of comparisons we perform. In particular, I am only going to compare graduate students whose last name begins with the letter \"H\". I will also limit the NSF researchers to those located in California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Subset the uc data, taking those last names start with \"H\"\n",
    "last_name_startswith_h = uc_name[\"LastName\"].apply(lambda s: s[0] == \"H\")\n",
    "uc_h = uc_name[startswith_h]\n",
    "\n",
    "# http://stackoverflow.com/questions/17957890/pandas-select-from-dataframe-using-startswith#comment26265963_17958424\n",
    "\n",
    "# Take Californian nsf records for comparison, also we only look at last names starting with \"H\"\n",
    "nsf_ca = nsf[(nsf['StateCode'] == \"CA\")]\n",
    "startswith_h = nsf_ca[\"LastName\"].apply(lambda s: type(s) == str and s[0].upper() == \"H\")\n",
    "nsf_h = nsf_ca[startswith_h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell takes a minute to execute\n",
    "\n",
    "# Create an empty list to save the outputs\n",
    "potential_matches = []\n",
    "\n",
    "# Loop over uc data frame by lines\n",
    "for _, uc_row in uc_h.iterrows():\n",
    "    # Store ID, FirstName and LastName of uc\n",
    "    rec1 = (uc_row[\"ID\"], uc_row[\"FirstName\"], uc_row[\"LastName\"])\n",
    "    # Loop over nsf data frame by lines\n",
    "    for nsf_ix, nsf_row in nsf_h.iterrows():    \n",
    "        # Store ID, FirstName and LastName of nsf\n",
    "        rec2 = (nsf_ix, nsf_row[\"FirstName\"], nsf_row[\"LastName\"])\n",
    "        # Calculate the match score of each pair of records\n",
    "        score = match_score(rec1, rec2)\n",
    "        # Save those pairs with score equal to or greater than 0.5\n",
    "        if score >= 0.5:\n",
    "            potential_matches.append((score, rec1, rec2))\n",
    "            \n",
    "# Sort the output so the best matches appear at the top\n",
    "potential_matches = sorted(potential_matches, key=lambda x: x[0], reverse=True)\n",
    "potential_matches\n",
    "\n",
    "# How did we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References & Further Readings\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "### Parsing\n",
    "\n",
    "* Python online ducumentation: https://docs.python.org/2/library/string.html#deprecated-string-functions\n",
    "* Python 2.7 Tutorial(Splitting and Joining Strings): http://www.pitt.edu/~naraehan/python2/split_join.html\n",
    "\n",
    "### Regular Expression\n",
    "\n",
    "* Python documentation: https://docs.python.org/2/library/re.html#regular-expression-syntax\n",
    "* Online regular expression tester (good for learning): http://regex101.com/\n",
    "\n",
    "### String Comparators\n",
    "\n",
    "* GitHub page of jellyfish: https://github.com/jamesturk/jellyfish\n",
    "* Different distances that measure the differences between strings:\n",
    "    - Levenshtein distance: https://en.wikipedia.org/wiki/Levenshtein_distance\n",
    "    - DamerauLevenshtein distance: https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance\n",
    "    - JaroWinkler distance: https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance\n",
    "    - Hamming distance: https://en.wikipedia.org/wiki/Hamming_distance\n",
    "    - Match rating approach: https://en.wikipedia.org/wiki/Match_rating_approach\n",
    "\n",
    "### Fellegi-Sunter Record Linkage \n",
    "\n",
    "* Introduction to Probabilistic Record Linkage: http://www.bristol.ac.uk/media-library/sites/cmm/migrated/documents/problinkage.pdf\n",
    "* Paper Review: https://www.cs.umd.edu/class/spring2012/cmsc828L/Papers/HerzogEtWires10.pdf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
