{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Topic Modeling Using MALLET\n",
    "\n",
    "For this exercise, we will be using data contained in the \"homework\" database on the Big Data for Social Science Class Server. This notebook will walk you through topic modeling NIH abstracts using [MALLET.](http://mallet.cs.umass.edu/topics.php)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Initialization](#Initialization)\n",
    "\n",
    "    - [Imports](#Imports)\n",
    "    - [The `terminal` function](#The-terminal-function)\n",
    "\n",
    "- [Getting Data](#Getting-Data)\n",
    "\n",
    "    - [Exercise 1](#Exercise-1)\n",
    "\n",
    "- [Generating Topics](#Generating-Topics)\n",
    "\n",
    "    - [Exercise 2](#Exercise-2)\n",
    "    - [Exercise 3](#Exercise-3)\n",
    "    - [Exercise 4 - Identifying Topics from Word Clusters](#Exercise-4---Identifying-Topics-from-Word-Clusters)\n",
    "\n",
    "- [Inferring Topics - Extra Credit](#Inferring-Topics---Extra-Credit)\n",
    "\n",
    "    - [Exercise 5 - Extra Credit](#Exercise-5---Extra-Credit)\n",
    "\n",
    "- [Resources for Topic Modeling](#Resources-for-Topic-Modeling)\n",
    "\n",
    "## Initialization\n",
    "\n",
    "* Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Before we begin, we'll need to run the following code cells, one of which imports Python libraries we'll be using, and one which defines a function, `terminal()`, that we'll use to run commands on the server.\n",
    "\n",
    "### Imports\n",
    "\n",
    "* Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Please run the following code cells before proceeding.  The first takes care of importing packages we'll use and pre-loading some data that the Natural Language Toolkit (NLTK) needs for processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing the modules we will use in this workbook\n",
    "from subprocess import Popen, PIPE\n",
    "import os\n",
    "import MySQLdb\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# download some nltk resources\n",
    "nltk.download( \"punkt\" )\n",
    "nltk.download( \"stopwords\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `terminal` function\n",
    "\n",
    "* Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Python can send commands to the operating system's command shell, allowing you to run operating system level commands without having to leave an iPython notebook.  For your convenience, a terminal() function is defined below that accepts a terminal command, broken on spaces into a list of command tokens, runs that command, then outputs the results to a text file.\n",
    "\n",
    "The terminal() function’s signature is:\n",
    "\n",
    "    terminal( commandTokenList, outputFile = \"temp.txt\" )\n",
    "\n",
    "WHERE:\n",
    "\n",
    "- **_commandTokenList_** - is the full operating system command line command you want to run, broken into a list of string tokens on each space within the command (spaces not included in the list).  Examples:\n",
    "\n",
    "    - command: `mkdir temp`\n",
    "    \n",
    "        - commandTokenList: [ 'mkdir', 'temp' ]\n",
    "        \n",
    "    - command: `ls -al | grep temp`\n",
    "    \n",
    "        - commandTokenList: [ 'ls', '-al', '|', 'grep', 'temp' ]\n",
    "        \n",
    "    - if there is a back slash ( \"\\\" ) in the middle of a command to show that it continues on the next line, you do not need to include that back slash as a token in the list.\n",
    "    - there are multiple ways to generate this list of tokens:\n",
    "    \n",
    "        - make a static list (as seen above):\n",
    "        \n",
    "                command_token_list = [ 'mkdir', 'temp' ]\n",
    "\n",
    "        - place your command in a string variable, then use `string.split()` to break the string into a list on spaces:\n",
    "        \n",
    "                my_command = \"ls -al | grep temp\"\n",
    "                command_token_list = my_command.split()\n",
    "                \n",
    "        - build up list of tokens one by one:\n",
    "        \n",
    "                command_token_list = []\n",
    "                command_token_list.append( \"ls\" )\n",
    "                command_token_list.append( \"-al\" )\n",
    "                command_token_list.append( \"|\" )\n",
    "                command_token_list.append( \"grep\" )\n",
    "                command_token_list.append( \"temp\" )\n",
    "\n",
    "- **_outputFile_** - is an optional parameter that tells the `terminal()` function where you want to store output from the command.  If you do not specify a file in the call to `terminal()` in the function call, it will write to a file named \"`temp.txt`\" in the current directory.\n",
    "\n",
    "**Note:** The output of this function attempts to capture whether there was an error or not based on where the command sent its output (either the standard or error output streams).  Terminal commands deal with output in many different ways, however, including sometimes sending error output to the standard output stream, and sometimes sending non-error logging to the error stream.  If there is output, the terminal command will tell you to look at the output file, then will output tags at the end of the message to tell you where the output is from:\n",
    "\n",
    "- \"`[err]`\" = error stream\n",
    "- \"`[standard]`\" = standard stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and, defining our terminal() function, writes the output to temp.txt,\n",
    "#    unless you change the filename.\n",
    "def terminal( commandTokenList, outputFile = \"temp.txt\" ):\n",
    "    \n",
    "    # return reference\n",
    "    status_OUT = \"\"\n",
    "\n",
    "    fwrite = open(outputFile, 'w')\n",
    "    pipe = Popen( commandTokenList, stdout = PIPE, stderr = PIPE )\n",
    "    text, err = pipe.communicate()    \n",
    "    text = text.decode('ascii', 'ignore')\n",
    "    err = err.decode('ascii', 'ignore')\n",
    "    if len(text) > 2:\n",
    "        fwrite.write(text)\n",
    "    elif len(err) > 2:\n",
    "        fwrite.write(err)\n",
    "    else:\n",
    "        print(\"No output returned\")\n",
    "    \n",
    "    # status\n",
    "    status_OUT = \"terminal() call complete\"\n",
    "\n",
    "    # got any output at all?\n",
    "    if ( ( ( err is not None ) and ( err != \"\" ) ) or ( ( text is not None ) and ( text != \"\" ) ) ):\n",
    "\n",
    "        # yes.  Add a note to look at output file.\n",
    "        status_OUT += \" - see \" + outputFile + \" for more details\"\n",
    "    \n",
    "        # error output?\n",
    "        if ( ( err is not None ) and ( err != \"\" ) ):\n",
    "    \n",
    "            # append a flag\n",
    "            status_OUT += \" [err]\"\n",
    "    \n",
    "        #-- END check for error messages. --#\n",
    "    \n",
    "        # standard output?\n",
    "        if ( ( text is not None ) and ( text != \"\" ) ):\n",
    "            \n",
    "            status_OUT += \" [standard]\"\n",
    "            \n",
    "        #-- END check to see if any output at all. --#\n",
    "    \n",
    "    #-- END check for status message --#\n",
    "    \n",
    "    return status_OUT\n",
    "    \n",
    "#-- END function terminal() --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data\n",
    "\n",
    "* Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We will be using a sample of abstracts from NIH grants stored in the 'TextAnalysis' table in the 'homework' database to explore automated text analysis.\n",
    "\n",
    "This table was created by taking a sample of abstracts from the grants located in the broader 'umetricsgrants' database.\n",
    "\n",
    "For text analysis, we'll be automatically deriving a list of topics based on these abstracts using MALLET, a Java based text analysis tool that makes topic modeling very easy.  MALLET is primarily a command line tool and requires a specific format for its data.  We'll use our `terminal()` function to run it, and we'll be creating appropriately formatted text files for each abstract as part of this exercise.  You can read more about importing data into MALLET [on the MALLET web site's \"Importing Data\" page.](http://mallet.cs.umass.edu/import.php)\n",
    "\n",
    "Let us first create a temporary directory in our home folder using the `terminal()` function.\n",
    "\n",
    "The following `terminal()` call will invoke the \"`mkdir temp`\" command to make a temporary directory named \"`temp`\" in your current working directory, storing output in the default file - \"`temp.txt`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "command_token_list = ['mkdir', 'temp']\n",
    "terminal( command_token_list )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get ready to retrieve the abstracts and the ids of the grants with which they are associated, and then store each abstract in a file with the grant id as the filename.\n",
    "\n",
    "First, we'll need to create a database connection to the \"`homework`\" database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create MySQL connection\n",
    "user = \"\"\n",
    "password = \"\"\n",
    "database = \"homework\"\n",
    "\n",
    "# invoke the connect() function, passing parameters in variables.\n",
    "db = MySQLdb.connect( user = user, passwd = password, db = database )\n",
    "\n",
    "# output basic database connection info.\n",
    "print( db )\n",
    "\n",
    "# create a database cursor.\n",
    "cursor = db.cursor( MySQLdb.cursors.DictCursor )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set up a few functions that we'll use in this exercise.  The first is `writeFile()`, a function that takes in a filename and text, and creates a new file populated with the text.  Run the cell below to define `writeFile()`.\n",
    "\n",
    "The writeFile() function’s signature is:\n",
    "\n",
    "    writeFile( filename, data )\n",
    "\n",
    "WHERE:\n",
    "\n",
    "- **_filename_** - is the name of the file you want to output, optionally combined with the path where you want the file to be output (if you are not writing to the current directory).\n",
    "- **_data_** - is the data you want to be stored in the file.\n",
    "\n",
    "It doesn't return anything.  If there are problems writing the file, Python will throw exceptions.  If no exceptions and the file is where you expect it to be and contains what you'd expect, then success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeFile(filename, data):\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        \n",
    "        f.write(str(data))\n",
    "        \n",
    "    #-- END with (automatically closes file) --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also wrote a function to do some initial cleaning of the abstract text - `cleanAbstract()`.\n",
    "\n",
    "The `cleanAbstract()` function’s signature is:\n",
    "\n",
    "    cleanAbstract( text )\n",
    "\n",
    "WHERE:\n",
    "\n",
    "- **_text_** - is the text of an abstract you want to clean.\n",
    "\n",
    "This function:\n",
    "\n",
    "- accepts an abstract's text\n",
    "- removes words that would be very common in NIH abstracts, because we dont want them to bias the results\n",
    "- removes stopwords (MALLET can also do that)\n",
    "- removes punctuation\n",
    "- returns the resulting cleaned string\n",
    "\n",
    "Run the cell below to define `cleanAbstract()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanAbstract(text):\n",
    "    \n",
    "    # common words to remove\n",
    "    commonWords = ['study', 'project', 'experiment', 'abstract', 'description', 'studies', \\\n",
    "                  'abstracts', 'projects', 'experiments', 'descriptions']\n",
    "\n",
    "    # remove white space.\n",
    "    text = re.sub('[\\n\\t\\r\\f]+', '', text)\n",
    "    \n",
    "    # convert to all lower case.\n",
    "    text = text.lower()\n",
    "    \n",
    "    # break text up into tokens (words)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # retrieve list of stop words.\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    # remove stop words from list of tokens.\n",
    "    tokens = [t for t in tokens if t not in stop]\n",
    "\n",
    "    # remove punctuation from tokens\n",
    "    exclude = set(string.punctuation)\n",
    "    tokenNew=[]\n",
    "    for s in tokens:\n",
    "        snew = ''.join(ch for ch in s if ch not in exclude)\n",
    "        if snew!=\"\":\n",
    "            tokenNew.append(snew)\n",
    "\n",
    "    # remove common words\n",
    "    tokenNew = [t for t in tokenNew if t not in commonWords]\n",
    "\n",
    "    # tie tokens back together.\n",
    "    abstract  = ' '.join(t for t in tokenNew)\n",
    "\n",
    "    return abstract\n",
    "\n",
    "#-- END function cleanAbstract() --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "* Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Retrieve the abstracts one by one from the database and write them to text files in the temp directory. For your convenience, the writeFile() function has already been created. You just need to call it with the path where you want the file stored and the contents of the abstract.\n",
    "\n",
    "You'll be reading the grant application abstracts from the table \"`TextAnalysis`\" in the database \"`homework`\", to which we connected above.  The table `TextAnalysis` has two columns: \"`APPLICATION_ID`\", the ID of the application with which a given abstract was associated; and \"`ABSTRACT_TEXT`\", the full text of that application's abstract.\n",
    "\n",
    "Create a query that SELECTs all of the records in TextAnalysis, and then for each row in the database, retrieve the application ID (column \"`APPLICATION_ID`\") and abstract (column \"`ABSTRACT_TEXT`\") from the row, clean the abstract using the \"`cleanAbstract()`\" function, then write the abstract out as the contents of a text file using the \"`writeFile()`\" function.\n",
    "\n",
    "When writing files, write each to the `temp` directory we created inside the current directory (path is \"./temp\").  Set the name of the files to the application ID from their grant (stored in the \"`APPLICATION_ID`\" column in the `TextAnalysis` database table), followed by \".txt\".\n",
    "\n",
    "So, the combined path and name for a given file (passed to \"`writeFile()`\" in the \"`filename`\" parameter) should be:\n",
    "\n",
    "    ./temp/<application_id>.txt\n",
    "    \n",
    "Again, remember to clean the text using `cleanAbstract()` before you write it out to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "TA_1",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# First create the query that you need to get the abstracts, excluding null abstracts\n",
    "query = 'SELECT * FROM homework.TextAnalysis WHERE ABSTRACT_TEXT IS NOT NULL ORDER BY APPLICATION_ID LIMIT 1000;'\n",
    "\n",
    "#Execute the query\n",
    "cursor.execute(query)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "#Fetch the results one by one and write them to a file\n",
    "row = cursor.fetchone()\n",
    "while (row is not None):\n",
    "    ID = row['APPLICATION_ID']\n",
    "    abstract = row['ABSTRACT_TEXT']\n",
    "    abstract = cleanAbstract(abstract)\n",
    "    filename = './temp/' + str(ID) + \".txt\"\n",
    "    writeFile(filename, abstract)\n",
    "    row = cursor.fetchone()\n",
    "### END SOLUTION\n",
    "\n",
    "# clean up\n",
    "cursor.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "TA_1_Test1",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test to see if file was successfully written\n",
    "f = open('./temp/2887634.txt', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Topics\n",
    "\n",
    "* Back to the [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created a number of .txt files in the temp directory, each of which contains a single abstract.  We will be using the set of these abstracts together as a corpus of data for machine learning.\n",
    "\n",
    "Our next task is to transform these individual files into a single file in MALLET format. To achieve this, we will use MALLET's import command. The import command can read in an entire directory, turn it into a MALLET file, and can also strip out common english stopwords. Our command will look something like this:\n",
    "\n",
    "    /bin/mallet/bin/mallet import-dir --input path/to/temp/directory --output data.mallet --keep-sequence --remove-stopwords\n",
    "\n",
    "Lets break down this command into each of the separate tokens it contains (where tokens are words separated by spaces):\n",
    "\n",
    "- **`/bin/mallet/bin/mallet`** ==> is the path to the MALLET program\n",
    "- **`import-dir`** ==> the first argument to the program mallet specifies what command the program is being asked to do.  The `import-dir` command tells MALLET to import an entire directory of files into a MALLET data file.\n",
    "- **`--input`** ==> \"--\" are used in MALLET to signify parameter names, usually followed by a space and a parameter value.  `--input` is a parameter used to tell MALLET the directory in which the corpus of data is located.\n",
    "- **`/path/to/temp/directory`** ==> Path to the directory that contains the corpus of data (the value for the parameter `--input`).\n",
    "- **`--output`** ==> tells MALLET where to store the output\n",
    "- **`data.mallet`** ==> name of file we'll store the MALLET data in (the value for the parameter `--outout`).\n",
    "- **`--keep-sequence`** ==> parameter that tells MALLET to keep the original texts in the order in which they were listed in the directory.  This is an example of a parameter that doesn't have an associated value.\n",
    "- **`--remove-stopwords`** ==> parameter that tells MALLET to remove common english stopwords like \"a\", \"an\", and \"the\".  Another parameter with no subsequent value.\n",
    "\n",
    "If you want help with the options available for a given mallet command, you can ask mallet for those options.  To do this, at the command line, run mallet with the command whose options you want to see, followed by \"`--help`\".  So, for example, to see the options for the \"`import-dir`\" command, at the server's unix terminal prompt, you'd run:\n",
    "\n",
    "    /bin/mallet/bin/mallet import-dir --help\n",
    "    \n",
    "This will output a list of options, what each should contain and what the default is should that option not be specified.  Example output:\n",
    "\n",
    "    jmorgan@ip-172-31-36-239:~$ /bin/mallet/bin/mallet import-dir --help\n",
    "    A tool for creating instance lists of FeatureVectors or FeatureSequences from text documents.\n",
    "\n",
    "    --help TRUE|FALSE\n",
    "      Print this command line option usage information.  Give argument of TRUE for longer documentation\n",
    "      Default is false\n",
    "    --prefix-code 'JAVA CODE'\n",
    "      Java code you want run before any other interpreted code.  Note that the text is interpreted without modification, so unlike some other Java code options, you need to include any necessary 'new's when creating objects.\n",
    "      Default is null\n",
    "    --config FILE\n",
    "      Read command option values from a file\n",
    "      Default is null\n",
    "    --input DIR...\n",
    "      The directories containing text files to be classified, one directory per class\n",
    "      Default is (null)\n",
    "    --output FILE\n",
    "      Write the instance list to this file; Using - indicates stdout.\n",
    "      Default is text.vectors\n",
    "    ...\n",
    "    \n",
    "If an option is not present in the list, it isn’t supported for the specified command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "* Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now use the `terminal()` function to run the MALLET `import-dir` command on your \"`./temp`\" directory.  Remember, the `terminal()` function accepts a list of arguments, with the command to be run the first item in the list, and then subsequent details of the command after, with each space-delimited part of the command an item in this list.  Given the above breakdown of the `import-dir` command, break that command into a list of arguments and invoke the command using `terminal()`, reading from your \"`./temp`\" directory and outputting the resulting MALLET data file to \"`data.mallet`\".\n",
    "\n",
    "- For help with available options for the mallet \"`import-dir`\" command, run the following at the command line:\n",
    "\n",
    "        /bin/mallet/bin/mallet import-dir --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "TA_2",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# store argument list in args[]\n",
    "args = None\n",
    "\n",
    "# Create list of command words in \"args\".\n",
    "### BEGIN SOLUTION\n",
    "args  = ['/bin/mallet/bin/mallet', 'import-dir', '--input',  './temp', '--output', \\\n",
    "         'data.mallet', '--keep-sequence', '--remove-stopwords']\n",
    "### END SOLUTION\n",
    "\n",
    "# run terminal() on args and print out results.\n",
    "print( terminal( args ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "TA_2_Test1",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test to see if file data.mallet was successfully written\n",
    "f = open('./data.mallet', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to your working directory now, you should find a file named \"`data.mallet`\".  This is the MALLET data file that we will use as input when we ask MALLET to generate topics based on a corpus of text.\n",
    "\n",
    "We will use the `train-topics` command in MALLET to generate our very own topic models.\n",
    "\n",
    "In the following example, we execute this command using its default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = ['/bin/mallet/bin/mallet', 'train-topics', '--input', 'data.mallet']\n",
    "print( terminal( args, \"mallet-train-default.txt\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command opens `data.mallet` and repeatedly runs MALLET's topic modeling algorithm on the corpus of documents in  it using default settings, printing out the results as it goes and using the results of each run to train a topic detection model to detect topics based on words used in texts in the corpus.\n",
    "\n",
    "The output of this command is captured by the `terminal()` command and written to a file whose name is in the second argument to the `terminal()` function (if you don't specify a name, it writes to `temp.txt`).  In the example above, we write the output to the file \"`mallet-train-default.txt`\".\n",
    "\n",
    "You can look a this output to get an idea of how MALLET works.  By default, MALLET prints out the keywords that make up the top 10 topics it detects, on every 50th iteration. A good high-level way to judge if the algorithm has converged is to look at this output. Each time it outputs topics, for each of the ten topics, MALLET outputs the topic ID, then a list of the keywords it associates with each topic.  If the keywords it outputs with each topic don't change much between iterations, it means that the model has converged.\n",
    "\n",
    "You can read more about the different options that can be used to fine tune the results [on the MALLET web site's \"Topic Modeling\" page.](http://mallet.cs.umass.edu/topics.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "* Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In the above example, we ran the base topic modeling algorithm but we didn't formally save the output anywhere.  If you look at the documentation pointed to above, it gives you different options to store the output.  Using this documentation as a guide, modify the args for the MALLET \"`train-topics`\" command so that it outputs:\n",
    "\n",
    "- topic keys, stored in file \"`topicKeys.txt`\"\n",
    "- topic composition of documents, stored in file \"`docTopics.txt`\"\n",
    "- a serialized MALLET topic trainer object, stored in file \"`model.mallet`\"\n",
    "\n",
    "Also add the option to:\n",
    "\n",
    "- enable hyperparameter optimization\n",
    "- increase the number of sampling iterations to 20,000\n",
    "- increase the number of topics to 20\n",
    "\n",
    "For help with available options for the mallet \"`train-topics`\" command, run the following at the command line:\n",
    "\n",
    "    /bin/mallet/bin/mallet train-topics --help\n",
    "\n",
    "**_NOTE: the topic modeling in this code cell could take a long time to complete - as long as there is an asterisk in the square brackets to its left (\"In [*]\"), it should still be running.  Give it some time._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "TA_3",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Modify the MALLET command to output topic keys, topic composition of documents, and a serialized MALLET topic trainer object.\n",
    "# Add the option to enable hyperparameter optimization, increase the number of sampling iterations to 20,000,\n",
    "# and increase the number of topics to 20.\n",
    "\n",
    "# store argument list in args[]\n",
    "args = None\n",
    "\n",
    "# Create list of command words in \"args\".\n",
    "### BEGIN SOLUTION\n",
    "args = ['/bin/mallet/bin/mallet', 'train-topics', '--input', 'data.mallet', '--optimize-interval', '10', \\\n",
    "        '--output-topic-keys', 'topicKeys.txt', '--output-doc-topics', 'docTopics.txt', '--num-topics', '20', \\\n",
    "       '--num-iterations', '20000']\n",
    "### END SOLUTION\n",
    "\n",
    "# run terminal() on args and print out results.\n",
    "print( terminal( args, \"mallet-train.txt\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "TA_3_Test1",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test to see if file data.mallet was successfully written\n",
    "f = open('./docTopics.txt', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at some results. In addition to the output log file \"`mallet-train.txt`\", your execution of MALLET should have resulted in two output files:\n",
    "\n",
    "- `topicKeys.txt` - a list of the topics detected in the abstracts, along with their weights and the words associated with each.\n",
    "- `docTopics.txt` - for each file in the corpus, lists each of the detected topics and a relavance score that indicates how likely it is that a given abstract relates to that topic.\n",
    "\n",
    "In `topicKeys.txt`, each topic gets a tab-delimited line in the file.  In a given topic's line, the first number is the numeric identifier of the topic (0, 1, 2, etc.), the second number gives an indication of the weight of that topic, and then after a tab, the line is completed with a list of the keywords associated with that topic.  An example (topic 0, weight 0.01502):\n",
    "\n",
    "    14\t0.05863\thealth care cancer data risk patients individuals outcomes genetic disease women testing aging older unreadable lung effects participants intervention community \n",
    "\n",
    "In `docTopics.txt`, each abstract, represented by its file path in the original directory, has a line in the file that lists the topics associated with that article and a relevance score for each topic, in order of decreasing relevance.  The relevance score runs from 0 to 1, where 0 is not at all relevant and 1 is perfectly related.  Example:\n",
    "\n",
    "    1\tfile:/home/jmorgan/nbgrader/courses/2015-fall-big_data/source/05.%20Text%20Analysis/./temp/6287560.txt\t14\t0.7923786992036733\t16\t0.10258021829417482\t11\t0.0414781353290313\t12\t0.04106461349323023\t17\t0.020602274154395795\t1\t3.1281525697325206E-4\t7\t2.5520223298362594E-4\t15\t2.0605374283754973E-4\t2\t1.4476870589053855E-4\t18\t1.3480683006926762E-4\t6\t1.216816985277047E-4\t10\t1.0585995154023939E-4\t9\t1.0155273496603757E-4\t5\t9.406085778593105E-5\t3\t8.487431366553085E-5\t19\t7.761498905199808E-5\t0\t7.675511604129449E-5\t13\t7.071872110668556E-5\t4\t5.972207592082584E-5\t8\t4.957229813413215E-5\n",
    "\n",
    "In this example, the abstract with ID 6287560 (found in the file path) is most highly related to topic 14 (our example above) with 0.792... relevance score.  In aggregate, this output could help you to find connections between documents based on these detected topics that you might not have otherwise noticed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Identifying Topics from Word Clusters\n",
    "\n",
    "* Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The topics our topic model generated based on the grant abstracts are clusters of words that it thinks are related.  Before we try to apply this model to an abstract not included in testing, however, we should first look more closly at these clusters of words.\n",
    "\n",
    "Just because a set of words are consistently found together across a corpus of training documents doesn't mean that the topic or category represented by those words is meaningful or useful.  In order for a topic or set of topics to be useful, one must understand what underlying concept each topic represents.\n",
    "\n",
    "There are many formal ways to take traits of something and figure out what underlying category sets of these traits represents (see, for example, [Grounded Theory](https://en.wikipedia.org/wiki/Grounded_theory)).\n",
    "\n",
    "Even informally, however, it is a good exercise to look at the sets of traits created by an algorithm like topic modeling and see if you can make sense of the topics it finds (and that is usually a step in any formal process, as well - \"face validity\" - do the topics make sense?).\n",
    "\n",
    "Based on the topics detected and output to `topicKeys.txt`, in the space below, list the topic ID, topic name, and brief description of any of the topics whose words suggest a substantive underlying concept or trait.  If none stand out, look for at least one or two that might, if you just removed a few words, or explain why you think none are meanigful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "TA_4-topic_names",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring Topics - Extra Credit\n",
    "\n",
    "* Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "You can use your newly trained model to infer topics for unseen documents. Since we got the first 1000 abstracts to train the model, let us use the model to infer topics on the 1001st abstract. We've already retrieved the text for it and placed it in the code cell that follows.  Run the code cell below to assign it to the variable \"`unseenAbstract`\", then create a file out of the abstract in a new work directory, named \"`./infer`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unseenAbstract = 'The overall goal of these experiments is to determine the effects of PD-linked mutations on \\\n",
    "the properties of the gene products and to use this information to discover methods to test possible explanations \\\n",
    "for pathogenicity.  We expect that this work will generate new therapeutic strategies for the treatment \\\n",
    "of Parkinson\\'s disease (PD).  Our emphasis will be on protein fibrillogenesis, since fibrillar cytoplasmic \\\n",
    "aggregates, or Lewy bodies, are diagnostic for PD and a major fibrillar component of Lewy bodies is also the \\\n",
    "product of a gene linked to early-onset PD.  Three mutations, in two different genes, encoding alpha-synuclein \\\n",
    "(alphaS) and ubiquitin C-hydrolase (UCH), have been linked to early-onset PD.  We have shown that the two alphaS \\\n",
    "mutations effect the oligomerization properties of the protein; both favor oligomerization.  It is a central goal \\\n",
    "of the proposed research to understand the structural basis for oligomerization and fibrillization and the \\\n",
    "relationship between this process and disease (the latter will require a collaboration between this project \\\n",
    "and project 3).  We are also very interested in the ubiquitin-dependent degradation of alphaS, especially \\\n",
    "since UCH may be involved in that pathway.  Finally, the possibility that mutant UCH may also be a fibrillogenic \\\n",
    "protein is under investigation.  Protein (UCH and alphaS) fibrillization will be a target for medium-throughput \\\n",
    "screening assays to be run in the Center core facility (Core B). A gene linked to juvenile-onset parkinsonism, \\\n",
    "parkin, will be the subject of future biochemical and biophysical investigations.  This protein contains an \\\n",
    "N-terminal ubiquitin homology domain, which suggests its involvement (like UCH) in the degradative process.  \\\n",
    "We intend to characterize wild-type and mutant forms of Parkin.  The fact that this disease is inherited in \\\n",
    "an autosomal recessive manner suggests that gain of function due to toxic oligomers may not be involved. '\n",
    "\n",
    "#Creating a new inference directory\n",
    "terminal(['mkdir', 'infer'])\n",
    "writeFile('./infer/' + \"6302892.txt\", unseenAbstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for topic inference can be found in [the Topic Modeling page at the MALLET site.](http://mallet.cs.umass.edu/topics.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 - Extra Credit\n",
    "\n",
    "* Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Use the MALLET documentation to set up a call to `mallet` to infer topics for the file we just created in the `./infer` folder.\n",
    "\n",
    "#### Step 1: store topic model in a file so it can be re-used\n",
    "\n",
    "First, we provide a mallet command that will re-run our topic model with an additional parameter to output a topic inference model specification to a file named \"`model.mallet`\".  Use the cell below to re-run the model training with the `--inferencer-filename` option set to output a re-usable inferencer to the file \"`model.mallet`\".\n",
    "\n",
    "**_NOTE: Each of the topic modeling steps could take a long time to complete - as long as there is an asterisk in the square brackets to its left (\"In [*]\"), the code in this cell should still be running on the server.  Give it some time._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will first need to rerun our model with the --inferencer-filename option\n",
    "args = ['/bin/mallet/bin/mallet', 'train-topics', '--input', 'data.mallet', '--optimize-interval', '10', \\\n",
    "        '--output-topic-keys', 'topicKeys.txt', '--output-doc-topics', 'docTopics.txt', '--num-topics', '20', \\\n",
    "       '--num-iterations', '20000', '--inferencer-filename', 'model.mallet']\n",
    "print( terminal( args, \"mallet-train-inferencer.txt\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Make a new mallet data file for the one abstract\n",
    "\n",
    "Next, you'll create and run a mallet command that executes the `import-dir` command to  create a new MALLET data file named \"`one.mallet`\", based on the contents of your `./infer` directory rather than your `./temp` directory, that will contain the article whose topics you want to infer.  Use the option `--use-pipe-from data.mallet` to specify our original data file as a training file for this corpus.  Make the `terminal()` function call for this command output to the file \"`mallet-infer-data.txt`\".\n",
    "\n",
    "As mentioned in the documenation, make sure that the new data is compatible with your training data. Use the option \"`--use-pipe-from [MALLET TRAINING FILE]`\" (without the square brackets around your training file) in the MALLET command import-dir to specify a training file.\n",
    "\n",
    "- For help with available options for the mallet \"`import-dir`\" command, run the following at the command line:\n",
    "\n",
    "        /bin/mallet/bin/mallet import-dir --help\n",
    "\n",
    "**_NOTE: Each of the topic modeling steps could take a long time to complete - as long as there is an asterisk in the square brackets to its left (\"In [*]\"), the code in this cell should still be running on the server.  Give it some time._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "TA_5-step2",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Use import-dir to pull our one file into a mallet data file.\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "args = ['/bin/mallet/bin/mallet', 'import-dir', '--input', './infer', '--output', 'one.mallet', '--use-pipe-from', \\\n",
    "        'data.mallet']\n",
    "print( terminal( args, \"mallet-infer-data.txt\" ) )\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Infer Topics in Abstract\n",
    "\n",
    "Finally, you'll create and run a mallet command that executes the `infer-topics` command, running for 10,000 iterations, using `one.mallet` as the input, the \"`model.mallet`\" model inferencer we created above as the inferencer, and that outputs the topics for the one abstract to a file named \"`inf-one.txt`\".  Make the `terminal()` function call for this command output to the file \"`mallet-infer-topics.txt`\".\n",
    "\n",
    "- For help with available options for the mallet \"`infer-topics`\" command, run the following at the command line:\n",
    "\n",
    "        /bin/mallet/bin/mallet infer-topics --help\n",
    "\n",
    "**_NOTE: Each of the topic modeling steps could take a long time to complete - as long as there is an asterisk in the square brackets to its left (\"In [*]\"), the code in this cell should still be running on the server.  Give it some time._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "TA_5-step3",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Use the infer-topics command to detect topics in the one abstract by running the\n",
    "#    abstract through th e\"model.mallet\" inferencer on it.\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "args = ['/bin/mallet/bin/mallet', 'infer-topics', '--input', 'one.mallet', '--inferencer', 'model.mallet', \\\n",
    "  '--output-doc-topics', 'inf-one.txt', '--num-iterations', '10000']\n",
    "print( terminal( args, \"mallet-infer-topics.txt\" ) )\n",
    "\n",
    "# Eventually, since all of the outputs of mallet are tab-delimited, could\n",
    "#    probably read and parse some of these files, verify the output inside.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "TA_5_Test1",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test to see if file inf-one.txt was successfully written\n",
    "f = open('./inf-one.txt', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources for Topic Modeling\n",
    "\n",
    "* Back to the [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Below you will find some tutorials and resources for topic modeling.\n",
    "- [General Introduction to Topic Modeling](https://www.cs.princeton.edu/~blei/papers/Blei2012.pdf)\n",
    "- [Topic Modeling for Humanists](http://www.scottbot.net/HIAL/?p=19113)\n",
    "- [Interpretation of Topic Models](http://www.umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
